{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Transfer Learning on Pre-trained SqueezeNet Model for Plant Disease Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* https://towardsdatascience.com/a-beginners-tutorial-on-building-an-ai-image-classifier-using-pytorch-6f85cb69cba7\n",
    "\n",
    "* https://pytorch.org/hub/pytorch_vision_squeezenet/\n",
    "* https://pytorch.org/docs/stable/torchvision/models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "new_dataset_path = \"D:\\\\Machine Learning Datasets\\\\Tomato Plant Disease Classification Dataset - Simplified\"\n",
    "trainset_path = os.path.join(new_dataset_path,'train')\n",
    "valset_path = os.path.join(new_dataset_path,'val')\n",
    "testset_path = os.path.join(new_dataset_path,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.ImageFolder(trainset_path, transform = transformations)\n",
    "val_set = datasets.ImageFolder(valset_path, transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size =32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model using torchvision.models as models library\n",
    "model = models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off training for their parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Conv2d(512, num_classes, kernel_size=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.AdaptiveAvgPool2d(13)\n",
    ")\n",
    "# model.forward = lambda x: model.classifier(model.features(x)).view(x.size(0), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=13)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "* https://discuss.pytorch.org/t/fine-tuning-squeezenet/3855/7\n",
    "* https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the error function using torch.nn as nn library\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Set the optimizer function using torch.optim as optim library\n",
    "# optimizer = optim.Adam(model.classifier.parameters())\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_sizes = {} \n",
    "dset_sizes['train'] = len(train_set)\n",
    "dset_sizes['val'] = len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 10416, 'val': 578}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # run for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            if phase == 'train':\n",
    "                phase_ = train_loader\n",
    "            else:\n",
    "                phase_ = val_loader\n",
    "\n",
    "            # Run through all data in mini batches\n",
    "            for data in phase_:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "                # calculating the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics for printing\n",
    "                #running_loss += loss.data[0]\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # save model if it performed better than\n",
    "            # any other previous model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 7.0746 Acc: 0.0688\n",
      "val Loss: 7.2094 Acc: 0.1817\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 7.1793 Acc: 0.1066\n",
      "val Loss: 7.2093 Acc: 0.1834\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 7.1412 Acc: 0.1022\n",
      "val Loss: 7.2093 Acc: 0.1834\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 7.1155 Acc: 0.0741\n",
      "val Loss: 7.2093 Acc: 0.1799\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 7.1018 Acc: 0.0853\n",
      "val Loss: 7.2094 Acc: 0.1782\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 7.1159 Acc: 0.0878\n",
      "val Loss: 7.2093 Acc: 0.1817\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 7.2042 Acc: 0.1256\n",
      "val Loss: 7.2094 Acc: 0.1817\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "train Loss: 7.1875 Acc: 0.1024\n",
      "val Loss: 7.2095 Acc: 0.1713\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 7.0767 Acc: 0.0559\n",
      "val Loss: 7.2029 Acc: 0.1436\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 6.9441 Acc: 0.0526\n",
      "val Loss: 7.2046 Acc: 0.0882\n",
      "\n",
      "Training complete in 12m 44s\n",
      "Best val Acc: 0.183391\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 326\n",
      "2 / 326\n",
      "3 / 326\n",
      "4 / 326\n",
      "5 / 326\n",
      "6 / 326\n",
      "7 / 326\n",
      "8 / 326\n",
      "9 / 326\n",
      "10 / 326\n",
      "11 / 326\n",
      "12 / 326\n",
      "13 / 326\n",
      "14 / 326\n",
      "15 / 326\n",
      "16 / 326\n",
      "17 / 326\n",
      "18 / 326\n",
      "19 / 326\n",
      "20 / 326\n",
      "21 / 326\n",
      "22 / 326\n",
      "23 / 326\n",
      "24 / 326\n",
      "25 / 326\n",
      "26 / 326\n",
      "27 / 326\n",
      "28 / 326\n",
      "29 / 326\n",
      "30 / 326\n",
      "31 / 326\n",
      "32 / 326\n",
      "33 / 326\n",
      "34 / 326\n",
      "35 / 326\n",
      "36 / 326\n",
      "37 / 326\n",
      "38 / 326\n",
      "39 / 326\n",
      "40 / 326\n",
      "41 / 326\n",
      "42 / 326\n",
      "43 / 326\n",
      "44 / 326\n",
      "45 / 326\n",
      "46 / 326\n",
      "47 / 326\n",
      "48 / 326\n",
      "49 / 326\n",
      "50 / 326\n",
      "51 / 326\n",
      "52 / 326\n",
      "53 / 326\n",
      "54 / 326\n",
      "55 / 326\n",
      "56 / 326\n",
      "57 / 326\n",
      "58 / 326\n",
      "59 / 326\n",
      "60 / 326\n",
      "61 / 326\n",
      "62 / 326\n",
      "63 / 326\n",
      "64 / 326\n",
      "65 / 326\n",
      "66 / 326\n",
      "67 / 326\n",
      "68 / 326\n",
      "69 / 326\n",
      "70 / 326\n",
      "71 / 326\n",
      "72 / 326\n",
      "73 / 326\n",
      "74 / 326\n",
      "75 / 326\n",
      "76 / 326\n",
      "77 / 326\n",
      "78 / 326\n",
      "79 / 326\n",
      "80 / 326\n",
      "81 / 326\n",
      "82 / 326\n",
      "83 / 326\n",
      "84 / 326\n",
      "85 / 326\n",
      "86 / 326\n",
      "87 / 326\n",
      "88 / 326\n",
      "89 / 326\n",
      "90 / 326\n",
      "91 / 326\n",
      "92 / 326\n",
      "93 / 326\n",
      "94 / 326\n",
      "95 / 326\n",
      "96 / 326\n",
      "97 / 326\n",
      "98 / 326\n",
      "99 / 326\n",
      "100 / 326\n",
      "101 / 326\n",
      "102 / 326\n",
      "103 / 326\n",
      "104 / 326\n",
      "105 / 326\n",
      "106 / 326\n",
      "107 / 326\n",
      "108 / 326\n",
      "109 / 326\n",
      "110 / 326\n",
      "111 / 326\n",
      "112 / 326\n",
      "113 / 326\n",
      "114 / 326\n",
      "115 / 326\n",
      "116 / 326\n",
      "117 / 326\n",
      "118 / 326\n",
      "119 / 326\n",
      "120 / 326\n",
      "121 / 326\n",
      "122 / 326\n",
      "123 / 326\n",
      "124 / 326\n",
      "125 / 326\n",
      "126 / 326\n",
      "127 / 326\n",
      "128 / 326\n",
      "129 / 326\n",
      "130 / 326\n",
      "131 / 326\n",
      "132 / 326\n",
      "133 / 326\n",
      "134 / 326\n",
      "135 / 326\n",
      "136 / 326\n",
      "137 / 326\n",
      "138 / 326\n",
      "139 / 326\n",
      "140 / 326\n",
      "141 / 326\n",
      "142 / 326\n",
      "143 / 326\n",
      "144 / 326\n",
      "145 / 326\n",
      "146 / 326\n",
      "147 / 326\n",
      "148 / 326\n",
      "149 / 326\n",
      "150 / 326\n",
      "151 / 326\n",
      "152 / 326\n",
      "153 / 326\n",
      "154 / 326\n",
      "155 / 326\n",
      "156 / 326\n",
      "157 / 326\n",
      "158 / 326\n",
      "159 / 326\n",
      "160 / 326\n",
      "161 / 326\n",
      "162 / 326\n",
      "163 / 326\n",
      "164 / 326\n",
      "165 / 326\n",
      "166 / 326\n",
      "167 / 326\n",
      "168 / 326\n",
      "169 / 326\n",
      "170 / 326\n",
      "171 / 326\n",
      "172 / 326\n",
      "173 / 326\n",
      "174 / 326\n",
      "175 / 326\n",
      "176 / 326\n",
      "177 / 326\n",
      "178 / 326\n",
      "179 / 326\n",
      "180 / 326\n",
      "181 / 326\n",
      "182 / 326\n",
      "183 / 326\n",
      "184 / 326\n",
      "185 / 326\n",
      "186 / 326\n",
      "187 / 326\n",
      "188 / 326\n",
      "189 / 326\n",
      "190 / 326\n",
      "191 / 326\n",
      "192 / 326\n",
      "193 / 326\n",
      "194 / 326\n",
      "195 / 326\n",
      "196 / 326\n",
      "197 / 326\n",
      "198 / 326\n",
      "199 / 326\n",
      "200 / 326\n",
      "201 / 326\n",
      "202 / 326\n",
      "203 / 326\n",
      "204 / 326\n",
      "205 / 326\n",
      "206 / 326\n",
      "207 / 326\n",
      "208 / 326\n",
      "209 / 326\n",
      "210 / 326\n",
      "211 / 326\n",
      "212 / 326\n",
      "213 / 326\n",
      "214 / 326\n",
      "215 / 326\n",
      "216 / 326\n",
      "217 / 326\n",
      "218 / 326\n",
      "219 / 326\n",
      "220 / 326\n",
      "221 / 326\n",
      "222 / 326\n",
      "223 / 326\n",
      "224 / 326\n",
      "225 / 326\n",
      "226 / 326\n",
      "227 / 326\n",
      "228 / 326\n",
      "229 / 326\n",
      "230 / 326\n",
      "231 / 326\n",
      "232 / 326\n",
      "233 / 326\n",
      "234 / 326\n",
      "235 / 326\n",
      "236 / 326\n",
      "237 / 326\n",
      "238 / 326\n",
      "239 / 326\n",
      "240 / 326\n",
      "241 / 326\n",
      "242 / 326\n",
      "243 / 326\n",
      "244 / 326\n",
      "245 / 326\n",
      "246 / 326\n",
      "247 / 326\n",
      "248 / 326\n",
      "249 / 326\n",
      "250 / 326\n",
      "251 / 326\n",
      "252 / 326\n",
      "253 / 326\n",
      "254 / 326\n",
      "255 / 326\n",
      "256 / 326\n",
      "257 / 326\n",
      "258 / 326\n",
      "259 / 326\n",
      "260 / 326\n",
      "261 / 326\n",
      "262 / 326\n",
      "263 / 326\n",
      "264 / 326\n",
      "265 / 326\n",
      "266 / 326\n",
      "267 / 326\n",
      "268 / 326\n",
      "269 / 326\n",
      "270 / 326\n",
      "271 / 326\n",
      "272 / 326\n",
      "273 / 326\n",
      "274 / 326\n",
      "275 / 326\n",
      "276 / 326\n",
      "277 / 326\n",
      "278 / 326\n",
      "279 / 326\n",
      "280 / 326\n",
      "281 / 326\n",
      "282 / 326\n",
      "283 / 326\n",
      "284 / 326\n",
      "285 / 326\n",
      "286 / 326\n",
      "287 / 326\n",
      "288 / 326\n",
      "289 / 326\n",
      "290 / 326\n",
      "291 / 326\n",
      "292 / 326\n",
      "293 / 326\n",
      "294 / 326\n",
      "295 / 326\n",
      "296 / 326\n",
      "297 / 326\n",
      "298 / 326\n",
      "299 / 326\n",
      "300 / 326\n",
      "301 / 326\n",
      "302 / 326\n",
      "303 / 326\n",
      "304 / 326\n",
      "305 / 326\n",
      "306 / 326\n",
      "307 / 326\n",
      "308 / 326\n",
      "309 / 326\n",
      "310 / 326\n",
      "311 / 326\n",
      "312 / 326\n",
      "313 / 326\n",
      "314 / 326\n",
      "315 / 326\n",
      "316 / 326\n",
      "317 / 326\n",
      "318 / 326\n",
      "319 / 326\n",
      "320 / 326\n",
      "321 / 326\n",
      "322 / 326\n",
      "323 / 326\n",
      "324 / 326\n",
      "325 / 326\n",
      "326 / 326\n",
      "1 / 19\n",
      "2 / 19\n",
      "3 / 19\n",
      "4 / 19\n",
      "5 / 19\n",
      "6 / 19\n",
      "7 / 19\n",
      "8 / 19\n",
      "9 / 19\n",
      "10 / 19\n",
      "11 / 19\n",
      "12 / 19\n",
      "13 / 19\n",
      "14 / 19\n",
      "15 / 19\n",
      "16 / 19\n",
      "17 / 19\n",
      "18 / 19\n",
      "19 / 19\n",
      "Accuracy:  0.10526315789473684\n",
      "Epoch: 0 \tTraining Loss: 6.070194 \tValidation Loss: 3.812655\n",
      "1 / 326\n",
      "2 / 326\n",
      "3 / 326\n",
      "4 / 326\n",
      "5 / 326\n",
      "6 / 326\n",
      "7 / 326\n",
      "8 / 326\n",
      "9 / 326\n",
      "10 / 326\n",
      "11 / 326\n",
      "12 / 326\n",
      "13 / 326\n",
      "14 / 326\n",
      "15 / 326\n",
      "16 / 326\n",
      "17 / 326\n",
      "18 / 326\n",
      "19 / 326\n",
      "20 / 326\n",
      "21 / 326\n",
      "22 / 326\n",
      "23 / 326\n",
      "24 / 326\n",
      "25 / 326\n",
      "26 / 326\n",
      "27 / 326\n",
      "28 / 326\n",
      "29 / 326\n",
      "30 / 326\n",
      "31 / 326\n",
      "32 / 326\n",
      "33 / 326\n",
      "34 / 326\n",
      "35 / 326\n",
      "36 / 326\n",
      "37 / 326\n",
      "38 / 326\n",
      "39 / 326\n",
      "40 / 326\n",
      "41 / 326\n",
      "42 / 326\n",
      "43 / 326\n",
      "44 / 326\n",
      "45 / 326\n",
      "46 / 326\n",
      "47 / 326\n",
      "48 / 326\n",
      "49 / 326\n",
      "50 / 326\n",
      "51 / 326\n",
      "52 / 326\n",
      "53 / 326\n",
      "54 / 326\n",
      "55 / 326\n",
      "56 / 326\n",
      "57 / 326\n",
      "58 / 326\n",
      "59 / 326\n",
      "60 / 326\n",
      "61 / 326\n",
      "62 / 326\n",
      "63 / 326\n",
      "64 / 326\n",
      "65 / 326\n",
      "66 / 326\n",
      "67 / 326\n",
      "68 / 326\n",
      "69 / 326\n",
      "70 / 326\n",
      "71 / 326\n",
      "72 / 326\n",
      "73 / 326\n",
      "74 / 326\n",
      "75 / 326\n",
      "76 / 326\n",
      "77 / 326\n",
      "78 / 326\n",
      "79 / 326\n",
      "80 / 326\n",
      "81 / 326\n",
      "82 / 326\n",
      "83 / 326\n",
      "84 / 326\n",
      "85 / 326\n",
      "86 / 326\n",
      "87 / 326\n",
      "88 / 326\n",
      "89 / 326\n",
      "90 / 326\n",
      "91 / 326\n",
      "92 / 326\n",
      "93 / 326\n",
      "94 / 326\n",
      "95 / 326\n",
      "96 / 326\n",
      "97 / 326\n",
      "98 / 326\n",
      "99 / 326\n",
      "100 / 326\n",
      "101 / 326\n",
      "102 / 326\n",
      "103 / 326\n",
      "104 / 326\n",
      "105 / 326\n",
      "106 / 326\n",
      "107 / 326\n",
      "108 / 326\n",
      "109 / 326\n",
      "110 / 326\n",
      "111 / 326\n",
      "112 / 326\n",
      "113 / 326\n",
      "114 / 326\n",
      "115 / 326\n",
      "116 / 326\n",
      "117 / 326\n",
      "118 / 326\n",
      "119 / 326\n",
      "120 / 326\n",
      "121 / 326\n",
      "122 / 326\n",
      "123 / 326\n",
      "124 / 326\n",
      "125 / 326\n",
      "126 / 326\n",
      "127 / 326\n",
      "128 / 326\n",
      "129 / 326\n",
      "130 / 326\n",
      "131 / 326\n",
      "132 / 326\n",
      "133 / 326\n",
      "134 / 326\n",
      "135 / 326\n",
      "136 / 326\n",
      "137 / 326\n",
      "138 / 326\n",
      "139 / 326\n",
      "140 / 326\n",
      "141 / 326\n",
      "142 / 326\n",
      "143 / 326\n",
      "144 / 326\n",
      "145 / 326\n",
      "146 / 326\n",
      "147 / 326\n",
      "148 / 326\n",
      "149 / 326\n",
      "150 / 326\n",
      "151 / 326\n",
      "152 / 326\n",
      "153 / 326\n",
      "154 / 326\n",
      "155 / 326\n",
      "156 / 326\n",
      "157 / 326\n",
      "158 / 326\n",
      "159 / 326\n",
      "160 / 326\n",
      "161 / 326\n",
      "162 / 326\n",
      "163 / 326\n",
      "164 / 326\n",
      "165 / 326\n",
      "166 / 326\n",
      "167 / 326\n",
      "168 / 326\n",
      "169 / 326\n",
      "170 / 326\n",
      "171 / 326\n",
      "172 / 326\n",
      "173 / 326\n",
      "174 / 326\n",
      "175 / 326\n",
      "176 / 326\n",
      "177 / 326\n",
      "178 / 326\n",
      "179 / 326\n",
      "180 / 326\n",
      "181 / 326\n",
      "182 / 326\n",
      "183 / 326\n",
      "184 / 326\n",
      "185 / 326\n",
      "186 / 326\n",
      "187 / 326\n",
      "188 / 326\n",
      "189 / 326\n",
      "190 / 326\n",
      "191 / 326\n",
      "192 / 326\n",
      "193 / 326\n",
      "194 / 326\n",
      "195 / 326\n",
      "196 / 326\n",
      "197 / 326\n",
      "198 / 326\n",
      "199 / 326\n",
      "200 / 326\n",
      "201 / 326\n",
      "202 / 326\n",
      "203 / 326\n",
      "204 / 326\n",
      "205 / 326\n",
      "206 / 326\n",
      "207 / 326\n",
      "208 / 326\n",
      "209 / 326\n",
      "210 / 326\n",
      "211 / 326\n",
      "212 / 326\n",
      "213 / 326\n",
      "214 / 326\n",
      "215 / 326\n",
      "216 / 326\n",
      "217 / 326\n",
      "218 / 326\n",
      "219 / 326\n",
      "220 / 326\n",
      "221 / 326\n",
      "222 / 326\n",
      "223 / 326\n",
      "224 / 326\n",
      "225 / 326\n",
      "226 / 326\n",
      "227 / 326\n",
      "228 / 326\n",
      "229 / 326\n",
      "230 / 326\n",
      "231 / 326\n",
      "232 / 326\n",
      "233 / 326\n",
      "234 / 326\n",
      "235 / 326\n",
      "236 / 326\n",
      "237 / 326\n",
      "238 / 326\n",
      "239 / 326\n",
      "240 / 326\n",
      "241 / 326\n",
      "242 / 326\n",
      "243 / 326\n",
      "244 / 326\n",
      "245 / 326\n",
      "246 / 326\n",
      "247 / 326\n",
      "248 / 326\n",
      "249 / 326\n",
      "250 / 326\n",
      "251 / 326\n",
      "252 / 326\n",
      "253 / 326\n",
      "254 / 326\n",
      "255 / 326\n",
      "256 / 326\n",
      "257 / 326\n",
      "258 / 326\n",
      "259 / 326\n",
      "260 / 326\n",
      "261 / 326\n",
      "262 / 326\n",
      "263 / 326\n",
      "264 / 326\n",
      "265 / 326\n",
      "266 / 326\n",
      "267 / 326\n",
      "268 / 326\n",
      "269 / 326\n",
      "270 / 326\n",
      "271 / 326\n",
      "272 / 326\n",
      "273 / 326\n",
      "274 / 326\n",
      "275 / 326\n",
      "276 / 326\n",
      "277 / 326\n",
      "278 / 326\n",
      "279 / 326\n",
      "280 / 326\n",
      "281 / 326\n",
      "282 / 326\n",
      "283 / 326\n",
      "284 / 326\n",
      "285 / 326\n",
      "286 / 326\n",
      "287 / 326\n",
      "288 / 326\n",
      "289 / 326\n",
      "290 / 326\n",
      "291 / 326\n",
      "292 / 326\n",
      "293 / 326\n",
      "294 / 326\n",
      "295 / 326\n",
      "296 / 326\n",
      "297 / 326\n",
      "298 / 326\n",
      "299 / 326\n",
      "300 / 326\n",
      "301 / 326\n",
      "302 / 326\n",
      "303 / 326\n",
      "304 / 326\n",
      "305 / 326\n",
      "306 / 326\n",
      "307 / 326\n",
      "308 / 326\n",
      "309 / 326\n",
      "310 / 326\n",
      "311 / 326\n",
      "312 / 326\n",
      "313 / 326\n",
      "314 / 326\n",
      "315 / 326\n",
      "316 / 326\n",
      "317 / 326\n",
      "318 / 326\n",
      "319 / 326\n",
      "320 / 326\n",
      "321 / 326\n",
      "322 / 326\n",
      "323 / 326\n",
      "324 / 326\n",
      "325 / 326\n",
      "326 / 326\n",
      "1 / 19\n",
      "2 / 19\n",
      "3 / 19\n",
      "4 / 19\n",
      "5 / 19\n",
      "6 / 19\n",
      "7 / 19\n",
      "8 / 19\n",
      "9 / 19\n",
      "10 / 19\n",
      "11 / 19\n",
      "12 / 19\n",
      "13 / 19\n",
      "14 / 19\n",
      "15 / 19\n",
      "16 / 19\n",
      "17 / 19\n",
      "18 / 19\n",
      "19 / 19\n",
      "Accuracy:  0.13815789473684212\n",
      "Epoch: 1 \tTraining Loss: 4.236854 \tValidation Loss: 3.643794\n",
      "1 / 326\n",
      "2 / 326\n",
      "3 / 326\n",
      "4 / 326\n",
      "5 / 326\n",
      "6 / 326\n",
      "7 / 326\n",
      "8 / 326\n",
      "9 / 326\n",
      "10 / 326\n",
      "11 / 326\n",
      "12 / 326\n",
      "13 / 326\n",
      "14 / 326\n",
      "15 / 326\n",
      "16 / 326\n",
      "17 / 326\n",
      "18 / 326\n",
      "19 / 326\n",
      "20 / 326\n",
      "21 / 326\n",
      "22 / 326\n",
      "23 / 326\n",
      "24 / 326\n",
      "25 / 326\n",
      "26 / 326\n",
      "27 / 326\n",
      "28 / 326\n",
      "29 / 326\n",
      "30 / 326\n",
      "31 / 326\n",
      "32 / 326\n",
      "33 / 326\n",
      "34 / 326\n",
      "35 / 326\n",
      "36 / 326\n",
      "37 / 326\n",
      "38 / 326\n",
      "39 / 326\n",
      "40 / 326\n",
      "41 / 326\n",
      "42 / 326\n",
      "43 / 326\n",
      "44 / 326\n",
      "45 / 326\n",
      "46 / 326\n",
      "47 / 326\n",
      "48 / 326\n",
      "49 / 326\n",
      "50 / 326\n",
      "51 / 326\n",
      "52 / 326\n",
      "53 / 326\n",
      "54 / 326\n",
      "55 / 326\n",
      "56 / 326\n",
      "57 / 326\n",
      "58 / 326\n",
      "59 / 326\n",
      "60 / 326\n",
      "61 / 326\n",
      "62 / 326\n",
      "63 / 326\n",
      "64 / 326\n",
      "65 / 326\n",
      "66 / 326\n",
      "67 / 326\n",
      "68 / 326\n",
      "69 / 326\n",
      "70 / 326\n",
      "71 / 326\n",
      "72 / 326\n",
      "73 / 326\n",
      "74 / 326\n",
      "75 / 326\n",
      "76 / 326\n",
      "77 / 326\n",
      "78 / 326\n",
      "79 / 326\n",
      "80 / 326\n",
      "81 / 326\n",
      "82 / 326\n",
      "83 / 326\n",
      "84 / 326\n",
      "85 / 326\n",
      "86 / 326\n",
      "87 / 326\n",
      "88 / 326\n",
      "89 / 326\n",
      "90 / 326\n",
      "91 / 326\n",
      "92 / 326\n",
      "93 / 326\n",
      "94 / 326\n",
      "95 / 326\n",
      "96 / 326\n",
      "97 / 326\n",
      "98 / 326\n",
      "99 / 326\n",
      "100 / 326\n",
      "101 / 326\n",
      "102 / 326\n",
      "103 / 326\n",
      "104 / 326\n",
      "105 / 326\n",
      "106 / 326\n",
      "107 / 326\n",
      "108 / 326\n",
      "109 / 326\n",
      "110 / 326\n",
      "111 / 326\n",
      "112 / 326\n",
      "113 / 326\n",
      "114 / 326\n",
      "115 / 326\n",
      "116 / 326\n",
      "117 / 326\n",
      "118 / 326\n",
      "119 / 326\n",
      "120 / 326\n",
      "121 / 326\n",
      "122 / 326\n",
      "123 / 326\n",
      "124 / 326\n",
      "125 / 326\n",
      "126 / 326\n",
      "127 / 326\n",
      "128 / 326\n",
      "129 / 326\n",
      "130 / 326\n",
      "131 / 326\n",
      "132 / 326\n",
      "133 / 326\n",
      "134 / 326\n",
      "135 / 326\n",
      "136 / 326\n",
      "137 / 326\n",
      "138 / 326\n",
      "139 / 326\n",
      "140 / 326\n",
      "141 / 326\n",
      "142 / 326\n",
      "143 / 326\n",
      "144 / 326\n",
      "145 / 326\n",
      "146 / 326\n",
      "147 / 326\n",
      "148 / 326\n",
      "149 / 326\n",
      "150 / 326\n",
      "151 / 326\n",
      "152 / 326\n",
      "153 / 326\n",
      "154 / 326\n",
      "155 / 326\n",
      "156 / 326\n",
      "157 / 326\n",
      "158 / 326\n",
      "159 / 326\n",
      "160 / 326\n",
      "161 / 326\n",
      "162 / 326\n",
      "163 / 326\n",
      "164 / 326\n",
      "165 / 326\n",
      "166 / 326\n",
      "167 / 326\n",
      "168 / 326\n",
      "169 / 326\n",
      "170 / 326\n",
      "171 / 326\n",
      "172 / 326\n",
      "173 / 326\n",
      "174 / 326\n",
      "175 / 326\n",
      "176 / 326\n",
      "177 / 326\n",
      "178 / 326\n",
      "179 / 326\n",
      "180 / 326\n",
      "181 / 326\n",
      "182 / 326\n",
      "183 / 326\n",
      "184 / 326\n",
      "185 / 326\n",
      "186 / 326\n",
      "187 / 326\n",
      "188 / 326\n",
      "189 / 326\n",
      "190 / 326\n",
      "191 / 326\n",
      "192 / 326\n",
      "193 / 326\n",
      "194 / 326\n",
      "195 / 326\n",
      "196 / 326\n",
      "197 / 326\n",
      "198 / 326\n",
      "199 / 326\n",
      "200 / 326\n",
      "201 / 326\n",
      "202 / 326\n",
      "203 / 326\n",
      "204 / 326\n",
      "205 / 326\n",
      "206 / 326\n",
      "207 / 326\n",
      "208 / 326\n",
      "209 / 326\n",
      "210 / 326\n",
      "211 / 326\n",
      "212 / 326\n",
      "213 / 326\n",
      "214 / 326\n",
      "215 / 326\n",
      "216 / 326\n",
      "217 / 326\n",
      "218 / 326\n",
      "219 / 326\n",
      "220 / 326\n",
      "221 / 326\n",
      "222 / 326\n",
      "223 / 326\n",
      "224 / 326\n",
      "225 / 326\n",
      "226 / 326\n",
      "227 / 326\n",
      "228 / 326\n",
      "229 / 326\n",
      "230 / 326\n",
      "231 / 326\n",
      "232 / 326\n",
      "233 / 326\n",
      "234 / 326\n",
      "235 / 326\n",
      "236 / 326\n",
      "237 / 326\n",
      "238 / 326\n",
      "239 / 326\n",
      "240 / 326\n",
      "241 / 326\n",
      "242 / 326\n",
      "243 / 326\n",
      "244 / 326\n",
      "245 / 326\n",
      "246 / 326\n",
      "247 / 326\n",
      "248 / 326\n",
      "249 / 326\n",
      "250 / 326\n",
      "251 / 326\n",
      "252 / 326\n",
      "253 / 326\n",
      "254 / 326\n",
      "255 / 326\n",
      "256 / 326\n",
      "257 / 326\n",
      "258 / 326\n",
      "259 / 326\n",
      "260 / 326\n",
      "261 / 326\n",
      "262 / 326\n",
      "263 / 326\n",
      "264 / 326\n",
      "265 / 326\n",
      "266 / 326\n",
      "267 / 326\n",
      "268 / 326\n",
      "269 / 326\n",
      "270 / 326\n",
      "271 / 326\n",
      "272 / 326\n",
      "273 / 326\n",
      "274 / 326\n",
      "275 / 326\n",
      "276 / 326\n",
      "277 / 326\n",
      "278 / 326\n",
      "279 / 326\n",
      "280 / 326\n",
      "281 / 326\n",
      "282 / 326\n",
      "283 / 326\n",
      "284 / 326\n",
      "285 / 326\n",
      "286 / 326\n",
      "287 / 326\n",
      "288 / 326\n",
      "289 / 326\n",
      "290 / 326\n",
      "291 / 326\n",
      "292 / 326\n",
      "293 / 326\n",
      "294 / 326\n",
      "295 / 326\n",
      "296 / 326\n",
      "297 / 326\n",
      "298 / 326\n",
      "299 / 326\n",
      "300 / 326\n",
      "301 / 326\n",
      "302 / 326\n",
      "303 / 326\n",
      "304 / 326\n",
      "305 / 326\n",
      "306 / 326\n",
      "307 / 326\n",
      "308 / 326\n",
      "309 / 326\n",
      "310 / 326\n",
      "311 / 326\n",
      "312 / 326\n",
      "313 / 326\n",
      "314 / 326\n",
      "315 / 326\n",
      "316 / 326\n",
      "317 / 326\n",
      "318 / 326\n",
      "319 / 326\n",
      "320 / 326\n",
      "321 / 326\n",
      "322 / 326\n",
      "323 / 326\n",
      "324 / 326\n",
      "325 / 326\n",
      "326 / 326\n",
      "1 / 19\n",
      "2 / 19\n",
      "3 / 19\n",
      "4 / 19\n",
      "5 / 19\n",
      "6 / 19\n",
      "7 / 19\n",
      "8 / 19\n",
      "9 / 19\n",
      "10 / 19\n",
      "11 / 19\n",
      "12 / 19\n",
      "13 / 19\n",
      "14 / 19\n",
      "15 / 19\n",
      "16 / 19\n",
      "17 / 19\n",
      "18 / 19\n",
      "19 / 19\n",
      "Accuracy:  0.12006578947368421\n",
      "Epoch: 2 \tTraining Loss: 4.106659 \tValidation Loss: 3.576050\n",
      "1 / 326\n",
      "2 / 326\n",
      "3 / 326\n",
      "4 / 326\n",
      "5 / 326\n",
      "6 / 326\n",
      "7 / 326\n",
      "8 / 326\n",
      "9 / 326\n",
      "10 / 326\n",
      "11 / 326\n",
      "12 / 326\n",
      "13 / 326\n",
      "14 / 326\n",
      "15 / 326\n",
      "16 / 326\n",
      "17 / 326\n",
      "18 / 326\n",
      "19 / 326\n",
      "20 / 326\n",
      "21 / 326\n",
      "22 / 326\n",
      "23 / 326\n",
      "24 / 326\n",
      "25 / 326\n",
      "26 / 326\n",
      "27 / 326\n",
      "28 / 326\n",
      "29 / 326\n",
      "30 / 326\n",
      "31 / 326\n",
      "32 / 326\n",
      "33 / 326\n",
      "34 / 326\n",
      "35 / 326\n",
      "36 / 326\n",
      "37 / 326\n",
      "38 / 326\n",
      "39 / 326\n",
      "40 / 326\n",
      "41 / 326\n",
      "42 / 326\n",
      "43 / 326\n",
      "44 / 326\n",
      "45 / 326\n",
      "46 / 326\n",
      "47 / 326\n",
      "48 / 326\n",
      "49 / 326\n",
      "50 / 326\n",
      "51 / 326\n",
      "52 / 326\n",
      "53 / 326\n",
      "54 / 326\n",
      "55 / 326\n",
      "56 / 326\n",
      "57 / 326\n",
      "58 / 326\n",
      "59 / 326\n",
      "60 / 326\n",
      "61 / 326\n",
      "62 / 326\n",
      "63 / 326\n",
      "64 / 326\n",
      "65 / 326\n",
      "66 / 326\n",
      "67 / 326\n",
      "68 / 326\n",
      "69 / 326\n",
      "70 / 326\n",
      "71 / 326\n",
      "72 / 326\n",
      "73 / 326\n",
      "74 / 326\n",
      "75 / 326\n",
      "76 / 326\n",
      "77 / 326\n",
      "78 / 326\n",
      "79 / 326\n",
      "80 / 326\n",
      "81 / 326\n",
      "82 / 326\n",
      "83 / 326\n",
      "84 / 326\n",
      "85 / 326\n",
      "86 / 326\n",
      "87 / 326\n",
      "88 / 326\n",
      "89 / 326\n",
      "90 / 326\n",
      "91 / 326\n",
      "92 / 326\n",
      "93 / 326\n",
      "94 / 326\n",
      "95 / 326\n",
      "96 / 326\n",
      "97 / 326\n",
      "98 / 326\n",
      "99 / 326\n",
      "100 / 326\n",
      "101 / 326\n",
      "102 / 326\n",
      "103 / 326\n",
      "104 / 326\n",
      "105 / 326\n",
      "106 / 326\n",
      "107 / 326\n",
      "108 / 326\n",
      "109 / 326\n",
      "110 / 326\n",
      "111 / 326\n",
      "112 / 326\n",
      "113 / 326\n",
      "114 / 326\n",
      "115 / 326\n",
      "116 / 326\n",
      "117 / 326\n",
      "118 / 326\n",
      "119 / 326\n",
      "120 / 326\n",
      "121 / 326\n",
      "122 / 326\n",
      "123 / 326\n",
      "124 / 326\n",
      "125 / 326\n",
      "126 / 326\n",
      "127 / 326\n",
      "128 / 326\n",
      "129 / 326\n",
      "130 / 326\n",
      "131 / 326\n",
      "132 / 326\n",
      "133 / 326\n",
      "134 / 326\n",
      "135 / 326\n",
      "136 / 326\n",
      "137 / 326\n",
      "138 / 326\n",
      "139 / 326\n",
      "140 / 326\n",
      "141 / 326\n",
      "142 / 326\n",
      "143 / 326\n",
      "144 / 326\n",
      "145 / 326\n",
      "146 / 326\n",
      "147 / 326\n",
      "148 / 326\n",
      "149 / 326\n",
      "150 / 326\n",
      "151 / 326\n",
      "152 / 326\n",
      "153 / 326\n",
      "154 / 326\n",
      "155 / 326\n",
      "156 / 326\n",
      "157 / 326\n",
      "158 / 326\n",
      "159 / 326\n",
      "160 / 326\n",
      "161 / 326\n",
      "162 / 326\n",
      "163 / 326\n",
      "164 / 326\n",
      "165 / 326\n",
      "166 / 326\n",
      "167 / 326\n",
      "168 / 326\n",
      "169 / 326\n",
      "170 / 326\n",
      "171 / 326\n",
      "172 / 326\n",
      "173 / 326\n",
      "174 / 326\n",
      "175 / 326\n",
      "176 / 326\n",
      "177 / 326\n",
      "178 / 326\n",
      "179 / 326\n",
      "180 / 326\n",
      "181 / 326\n",
      "182 / 326\n",
      "183 / 326\n",
      "184 / 326\n",
      "185 / 326\n",
      "186 / 326\n",
      "187 / 326\n",
      "188 / 326\n",
      "189 / 326\n",
      "190 / 326\n",
      "191 / 326\n",
      "192 / 326\n",
      "193 / 326\n",
      "194 / 326\n",
      "195 / 326\n",
      "196 / 326\n",
      "197 / 326\n",
      "198 / 326\n",
      "199 / 326\n",
      "200 / 326\n",
      "201 / 326\n",
      "202 / 326\n",
      "203 / 326\n",
      "204 / 326\n",
      "205 / 326\n",
      "206 / 326\n",
      "207 / 326\n",
      "208 / 326\n",
      "209 / 326\n",
      "210 / 326\n",
      "211 / 326\n",
      "212 / 326\n",
      "213 / 326\n",
      "214 / 326\n",
      "215 / 326\n",
      "216 / 326\n",
      "217 / 326\n",
      "218 / 326\n",
      "219 / 326\n",
      "220 / 326\n",
      "221 / 326\n",
      "222 / 326\n",
      "223 / 326\n",
      "224 / 326\n",
      "225 / 326\n",
      "226 / 326\n",
      "227 / 326\n",
      "228 / 326\n",
      "229 / 326\n",
      "230 / 326\n",
      "231 / 326\n",
      "232 / 326\n",
      "233 / 326\n",
      "234 / 326\n",
      "235 / 326\n",
      "236 / 326\n",
      "237 / 326\n",
      "238 / 326\n",
      "239 / 326\n",
      "240 / 326\n",
      "241 / 326\n",
      "242 / 326\n",
      "243 / 326\n",
      "244 / 326\n",
      "245 / 326\n",
      "246 / 326\n",
      "247 / 326\n",
      "248 / 326\n",
      "249 / 326\n",
      "250 / 326\n",
      "251 / 326\n",
      "252 / 326\n",
      "253 / 326\n",
      "254 / 326\n",
      "255 / 326\n",
      "256 / 326\n",
      "257 / 326\n",
      "258 / 326\n",
      "259 / 326\n",
      "260 / 326\n",
      "261 / 326\n",
      "262 / 326\n",
      "263 / 326\n",
      "264 / 326\n",
      "265 / 326\n",
      "266 / 326\n",
      "267 / 326\n",
      "268 / 326\n",
      "269 / 326\n",
      "270 / 326\n",
      "271 / 326\n",
      "272 / 326\n",
      "273 / 326\n",
      "274 / 326\n",
      "275 / 326\n",
      "276 / 326\n",
      "277 / 326\n",
      "278 / 326\n",
      "279 / 326\n",
      "280 / 326\n",
      "281 / 326\n",
      "282 / 326\n",
      "283 / 326\n",
      "284 / 326\n",
      "285 / 326\n",
      "286 / 326\n",
      "287 / 326\n",
      "288 / 326\n",
      "289 / 326\n",
      "290 / 326\n",
      "291 / 326\n",
      "292 / 326\n",
      "293 / 326\n",
      "294 / 326\n",
      "295 / 326\n",
      "296 / 326\n",
      "297 / 326\n",
      "298 / 326\n",
      "299 / 326\n",
      "300 / 326\n",
      "301 / 326\n",
      "302 / 326\n",
      "303 / 326\n",
      "304 / 326\n",
      "305 / 326\n",
      "306 / 326\n",
      "307 / 326\n",
      "308 / 326\n",
      "309 / 326\n",
      "310 / 326\n",
      "311 / 326\n",
      "312 / 326\n",
      "313 / 326\n",
      "314 / 326\n",
      "315 / 326\n",
      "316 / 326\n",
      "317 / 326\n",
      "318 / 326\n",
      "319 / 326\n",
      "320 / 326\n",
      "321 / 326\n",
      "322 / 326\n",
      "323 / 326\n",
      "324 / 326\n",
      "325 / 326\n",
      "326 / 326\n",
      "1 / 19\n",
      "2 / 19\n",
      "3 / 19\n",
      "4 / 19\n",
      "5 / 19\n",
      "6 / 19\n",
      "7 / 19\n",
      "8 / 19\n",
      "9 / 19\n",
      "10 / 19\n",
      "11 / 19\n",
      "12 / 19\n",
      "13 / 19\n",
      "14 / 19\n",
      "15 / 19\n",
      "16 / 19\n",
      "17 / 19\n",
      "18 / 19\n",
      "19 / 19\n",
      "Accuracy:  0.14802631578947367\n",
      "Epoch: 3 \tTraining Loss: 4.061716 \tValidation Loss: 3.463351\n",
      "1 / 326\n",
      "2 / 326\n",
      "3 / 326\n",
      "4 / 326\n",
      "5 / 326\n",
      "6 / 326\n",
      "7 / 326\n",
      "8 / 326\n",
      "9 / 326\n",
      "10 / 326\n",
      "11 / 326\n",
      "12 / 326\n",
      "13 / 326\n",
      "14 / 326\n",
      "15 / 326\n",
      "16 / 326\n",
      "17 / 326\n",
      "18 / 326\n",
      "19 / 326\n",
      "20 / 326\n",
      "21 / 326\n",
      "22 / 326\n",
      "23 / 326\n",
      "24 / 326\n",
      "25 / 326\n",
      "26 / 326\n",
      "27 / 326\n",
      "28 / 326\n",
      "29 / 326\n",
      "30 / 326\n",
      "31 / 326\n",
      "32 / 326\n",
      "33 / 326\n",
      "34 / 326\n",
      "35 / 326\n",
      "36 / 326\n",
      "37 / 326\n",
      "38 / 326\n",
      "39 / 326\n",
      "40 / 326\n",
      "41 / 326\n",
      "42 / 326\n",
      "43 / 326\n",
      "44 / 326\n",
      "45 / 326\n",
      "46 / 326\n",
      "47 / 326\n",
      "48 / 326\n",
      "49 / 326\n",
      "50 / 326\n",
      "51 / 326\n",
      "52 / 326\n",
      "53 / 326\n",
      "54 / 326\n",
      "55 / 326\n",
      "56 / 326\n",
      "57 / 326\n",
      "58 / 326\n",
      "59 / 326\n",
      "60 / 326\n",
      "61 / 326\n",
      "62 / 326\n",
      "63 / 326\n",
      "64 / 326\n",
      "65 / 326\n",
      "66 / 326\n",
      "67 / 326\n",
      "68 / 326\n",
      "69 / 326\n",
      "70 / 326\n",
      "71 / 326\n",
      "72 / 326\n",
      "73 / 326\n",
      "74 / 326\n",
      "75 / 326\n",
      "76 / 326\n",
      "77 / 326\n",
      "78 / 326\n",
      "79 / 326\n",
      "80 / 326\n",
      "81 / 326\n",
      "82 / 326\n",
      "83 / 326\n",
      "84 / 326\n",
      "85 / 326\n",
      "86 / 326\n",
      "87 / 326\n",
      "88 / 326\n",
      "89 / 326\n",
      "90 / 326\n",
      "91 / 326\n",
      "92 / 326\n",
      "93 / 326\n",
      "94 / 326\n",
      "95 / 326\n",
      "96 / 326\n",
      "97 / 326\n",
      "98 / 326\n",
      "99 / 326\n",
      "100 / 326\n",
      "101 / 326\n",
      "102 / 326\n",
      "103 / 326\n",
      "104 / 326\n",
      "105 / 326\n",
      "106 / 326\n",
      "107 / 326\n",
      "108 / 326\n",
      "109 / 326\n",
      "110 / 326\n",
      "111 / 326\n",
      "112 / 326\n",
      "113 / 326\n",
      "114 / 326\n",
      "115 / 326\n",
      "116 / 326\n",
      "117 / 326\n",
      "118 / 326\n",
      "119 / 326\n",
      "120 / 326\n",
      "121 / 326\n",
      "122 / 326\n",
      "123 / 326\n",
      "124 / 326\n",
      "125 / 326\n",
      "126 / 326\n",
      "127 / 326\n",
      "128 / 326\n",
      "129 / 326\n",
      "130 / 326\n",
      "131 / 326\n",
      "132 / 326\n",
      "133 / 326\n",
      "134 / 326\n",
      "135 / 326\n",
      "136 / 326\n",
      "137 / 326\n",
      "138 / 326\n",
      "139 / 326\n",
      "140 / 326\n",
      "141 / 326\n",
      "142 / 326\n",
      "143 / 326\n",
      "144 / 326\n",
      "145 / 326\n",
      "146 / 326\n",
      "147 / 326\n",
      "148 / 326\n",
      "149 / 326\n",
      "150 / 326\n",
      "151 / 326\n",
      "152 / 326\n",
      "153 / 326\n",
      "154 / 326\n",
      "155 / 326\n",
      "156 / 326\n",
      "157 / 326\n",
      "158 / 326\n",
      "159 / 326\n",
      "160 / 326\n",
      "161 / 326\n",
      "162 / 326\n",
      "163 / 326\n",
      "164 / 326\n",
      "165 / 326\n",
      "166 / 326\n",
      "167 / 326\n",
      "168 / 326\n",
      "169 / 326\n",
      "170 / 326\n",
      "171 / 326\n",
      "172 / 326\n",
      "173 / 326\n",
      "174 / 326\n",
      "175 / 326\n",
      "176 / 326\n",
      "177 / 326\n",
      "178 / 326\n",
      "179 / 326\n",
      "180 / 326\n",
      "181 / 326\n",
      "182 / 326\n",
      "183 / 326\n",
      "184 / 326\n",
      "185 / 326\n",
      "186 / 326\n",
      "187 / 326\n",
      "188 / 326\n",
      "189 / 326\n",
      "190 / 326\n",
      "191 / 326\n",
      "192 / 326\n",
      "193 / 326\n",
      "194 / 326\n",
      "195 / 326\n",
      "196 / 326\n",
      "197 / 326\n",
      "198 / 326\n",
      "199 / 326\n",
      "200 / 326\n",
      "201 / 326\n",
      "202 / 326\n",
      "203 / 326\n",
      "204 / 326\n",
      "205 / 326\n",
      "206 / 326\n",
      "207 / 326\n",
      "208 / 326\n",
      "209 / 326\n",
      "210 / 326\n",
      "211 / 326\n",
      "212 / 326\n",
      "213 / 326\n",
      "214 / 326\n",
      "215 / 326\n",
      "216 / 326\n",
      "217 / 326\n",
      "218 / 326\n",
      "219 / 326\n",
      "220 / 326\n",
      "221 / 326\n",
      "222 / 326\n",
      "223 / 326\n",
      "224 / 326\n",
      "225 / 326\n",
      "226 / 326\n",
      "227 / 326\n",
      "228 / 326\n",
      "229 / 326\n",
      "230 / 326\n",
      "231 / 326\n",
      "232 / 326\n",
      "233 / 326\n",
      "234 / 326\n",
      "235 / 326\n",
      "236 / 326\n",
      "237 / 326\n",
      "238 / 326\n",
      "239 / 326\n",
      "240 / 326\n",
      "241 / 326\n",
      "242 / 326\n",
      "243 / 326\n",
      "244 / 326\n",
      "245 / 326\n",
      "246 / 326\n",
      "247 / 326\n",
      "248 / 326\n",
      "249 / 326\n",
      "250 / 326\n",
      "251 / 326\n",
      "252 / 326\n",
      "253 / 326\n",
      "254 / 326\n",
      "255 / 326\n",
      "256 / 326\n",
      "257 / 326\n",
      "258 / 326\n",
      "259 / 326\n",
      "260 / 326\n",
      "261 / 326\n",
      "262 / 326\n",
      "263 / 326\n",
      "264 / 326\n",
      "265 / 326\n",
      "266 / 326\n",
      "267 / 326\n",
      "268 / 326\n",
      "269 / 326\n",
      "270 / 326\n",
      "271 / 326\n",
      "272 / 326\n",
      "273 / 326\n",
      "274 / 326\n",
      "275 / 326\n",
      "276 / 326\n",
      "277 / 326\n",
      "278 / 326\n",
      "279 / 326\n",
      "280 / 326\n",
      "281 / 326\n",
      "282 / 326\n",
      "283 / 326\n",
      "284 / 326\n",
      "285 / 326\n",
      "286 / 326\n",
      "287 / 326\n",
      "288 / 326\n",
      "289 / 326\n",
      "290 / 326\n",
      "291 / 326\n",
      "292 / 326\n",
      "293 / 326\n",
      "294 / 326\n",
      "295 / 326\n",
      "296 / 326\n",
      "297 / 326\n",
      "298 / 326\n",
      "299 / 326\n",
      "300 / 326\n",
      "301 / 326\n",
      "302 / 326\n",
      "303 / 326\n",
      "304 / 326\n",
      "305 / 326\n",
      "306 / 326\n",
      "307 / 326\n",
      "308 / 326\n",
      "309 / 326\n",
      "310 / 326\n",
      "311 / 326\n",
      "312 / 326\n",
      "313 / 326\n",
      "314 / 326\n",
      "315 / 326\n",
      "316 / 326\n",
      "317 / 326\n",
      "318 / 326\n",
      "319 / 326\n",
      "320 / 326\n",
      "321 / 326\n",
      "322 / 326\n",
      "323 / 326\n",
      "324 / 326\n",
      "325 / 326\n",
      "326 / 326\n",
      "1 / 19\n",
      "2 / 19\n",
      "3 / 19\n",
      "4 / 19\n",
      "5 / 19\n",
      "6 / 19\n",
      "7 / 19\n",
      "8 / 19\n",
      "9 / 19\n",
      "10 / 19\n",
      "11 / 19\n",
      "12 / 19\n",
      "13 / 19\n",
      "14 / 19\n",
      "15 / 19\n",
      "16 / 19\n",
      "17 / 19\n",
      "18 / 19\n",
      "19 / 19\n",
      "Accuracy:  0.13157894736842105\n",
      "Epoch: 4 \tTraining Loss: 3.946830 \tValidation Loss: 3.445337\n",
      "1 / 326\n",
      "2 / 326\n",
      "3 / 326\n",
      "4 / 326\n",
      "5 / 326\n",
      "6 / 326\n",
      "7 / 326\n",
      "8 / 326\n",
      "9 / 326\n",
      "10 / 326\n",
      "11 / 326\n",
      "12 / 326\n",
      "13 / 326\n",
      "14 / 326\n",
      "15 / 326\n",
      "16 / 326\n",
      "17 / 326\n",
      "18 / 326\n",
      "19 / 326\n",
      "20 / 326\n",
      "21 / 326\n",
      "22 / 326\n",
      "23 / 326\n",
      "24 / 326\n",
      "25 / 326\n",
      "26 / 326\n",
      "27 / 326\n",
      "28 / 326\n",
      "29 / 326\n",
      "30 / 326\n",
      "31 / 326\n",
      "32 / 326\n",
      "33 / 326\n",
      "34 / 326\n",
      "35 / 326\n",
      "36 / 326\n",
      "37 / 326\n",
      "38 / 326\n",
      "39 / 326\n",
      "40 / 326\n",
      "41 / 326\n",
      "42 / 326\n",
      "43 / 326\n",
      "44 / 326\n",
      "45 / 326\n",
      "46 / 326\n",
      "47 / 326\n",
      "48 / 326\n",
      "49 / 326\n",
      "50 / 326\n",
      "51 / 326\n",
      "52 / 326\n",
      "53 / 326\n",
      "54 / 326\n",
      "55 / 326\n",
      "56 / 326\n",
      "57 / 326\n",
      "58 / 326\n",
      "59 / 326\n",
      "60 / 326\n",
      "61 / 326\n",
      "62 / 326\n",
      "63 / 326\n",
      "64 / 326\n",
      "65 / 326\n",
      "66 / 326\n",
      "67 / 326\n",
      "68 / 326\n",
      "69 / 326\n",
      "70 / 326\n",
      "71 / 326\n",
      "72 / 326\n",
      "73 / 326\n",
      "74 / 326\n",
      "75 / 326\n",
      "76 / 326\n",
      "77 / 326\n",
      "78 / 326\n",
      "79 / 326\n",
      "80 / 326\n",
      "81 / 326\n",
      "82 / 326\n",
      "83 / 326\n",
      "84 / 326\n",
      "85 / 326\n",
      "86 / 326\n",
      "87 / 326\n",
      "88 / 326\n",
      "89 / 326\n",
      "90 / 326\n",
      "91 / 326\n",
      "92 / 326\n",
      "93 / 326\n",
      "94 / 326\n",
      "95 / 326\n",
      "96 / 326\n",
      "97 / 326\n",
      "98 / 326\n",
      "99 / 326\n",
      "100 / 326\n",
      "101 / 326\n",
      "102 / 326\n",
      "103 / 326\n",
      "104 / 326\n",
      "105 / 326\n",
      "106 / 326\n",
      "107 / 326\n",
      "108 / 326\n",
      "109 / 326\n",
      "110 / 326\n",
      "111 / 326\n",
      "112 / 326\n",
      "113 / 326\n",
      "114 / 326\n",
      "115 / 326\n",
      "116 / 326\n",
      "117 / 326\n",
      "118 / 326\n",
      "119 / 326\n",
      "120 / 326\n",
      "121 / 326\n",
      "122 / 326\n",
      "123 / 326\n",
      "124 / 326\n",
      "125 / 326\n",
      "126 / 326\n",
      "127 / 326\n",
      "128 / 326\n",
      "129 / 326\n",
      "130 / 326\n",
      "131 / 326\n",
      "132 / 326\n",
      "133 / 326\n",
      "134 / 326\n",
      "135 / 326\n",
      "136 / 326\n",
      "137 / 326\n",
      "138 / 326\n",
      "139 / 326\n",
      "140 / 326\n",
      "141 / 326\n",
      "142 / 326\n",
      "143 / 326\n",
      "144 / 326\n",
      "145 / 326\n",
      "146 / 326\n",
      "147 / 326\n",
      "148 / 326\n",
      "149 / 326\n",
      "150 / 326\n",
      "151 / 326\n",
      "152 / 326\n",
      "153 / 326\n",
      "154 / 326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-f38c6d1774bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Move to device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python37_cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python37_cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python37_cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python37_cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.cache\\torch\\hub\\pytorch_vision_v0.6.0\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m    134\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.cache\\torch\\hub\\pytorch_vision_v0.6.0\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = 0\n",
    "#     val_loss = 0\n",
    "#     accuracy = 0\n",
    "    \n",
    "#     # Training the model\n",
    "#     model.train()\n",
    "#     counter = 0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         # Move to device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         # Clear optimizers\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         output = model.forward(inputs)\n",
    "#         # Loss\n",
    "#         loss = criterion(output, labels)\n",
    "#         # Calculate gradients (backpropogation)\n",
    "#         loss.backward()\n",
    "#         # Adjust parameters based on gradients\n",
    "#         optimizer.step()\n",
    "#         # Add the loss to the training set's rnning loss\n",
    "#         train_loss += loss.item()*inputs.size(0)\n",
    "        \n",
    "#         # Print the progress of our training\n",
    "#         counter += 1\n",
    "#         print(counter, \"/\", len(train_loader))\n",
    "        \n",
    "#     # Evaluating the model\n",
    "#     model.eval()\n",
    "#     counter = 0\n",
    "#     # Tell torch not to calculate gradients\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             # Move to device\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             # Forward pass\n",
    "#             output = model.forward(inputs)\n",
    "#             # Calculate Loss\n",
    "#             valloss = criterion(output, labels)\n",
    "#             # Add loss to the validation set's running loss\n",
    "#             val_loss += valloss.item()*inputs.size(0)\n",
    "            \n",
    "#             # Since our model outputs a LogSoftmax, find the real \n",
    "#             # percentages by reversing the log function\n",
    "#             output = torch.exp(output)\n",
    "#             # Get the top class of the output\n",
    "#             top_p, top_class = output.topk(1, dim=1)\n",
    "#             # See how many of the classes were correct?\n",
    "#             equals = top_class == labels.view(*top_class.shape)\n",
    "#             # Calculate the mean (get the accuracy for this batch)\n",
    "#             # and add it to the running accuracy for this epoch\n",
    "#             accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "#             # Print the progress of our evaluation\n",
    "#             counter += 1\n",
    "#             print(counter, \"/\", len(val_loader))\n",
    "    \n",
    "#     # Get the average loss for the entire epoch\n",
    "#     train_loss = train_loss/len(train_loader.dataset)\n",
    "#     valid_loss = val_loss/len(val_loader.dataset)\n",
    "#     # Print out the information\n",
    "#     print('Accuracy: ', accuracy/len(val_loader))\n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
